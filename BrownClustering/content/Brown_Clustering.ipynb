{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mLVZQRk7hp2",
    "outputId": "e049f6ac-d7bc-453c-9b02-a7c6ab565dd4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frGYuYND7hfL"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P0eDLcz7r2f"
   },
   "outputs": [],
   "source": [
    "def get_synonyms(ofword):\n",
    "  from nltk.corpus import wordnet\n",
    "  synonyms = []\n",
    "\n",
    "  for syn in wordnet.synsets(ofword):\n",
    "    for l in syn.lemmas():\n",
    "      synonyms.append(l.name())\n",
    "\n",
    "  return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50W82TH97roA"
   },
   "outputs": [],
   "source": [
    "def get_antonyms(ofword):\n",
    "  from nltk.corpus import wordnet\n",
    "  antonyms = []\n",
    "\n",
    "  for syn in wordnet.synsets(ofword):\n",
    "    for l in syn.lemmas():\n",
    "      if l.antonyms():\n",
    "          antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "  return antonyms\n",
    "  # print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvFipBQB7wq1",
    "outputId": "9c9081ca-1070-48d7-bd1a-e5fa96481406"
   },
   "outputs": [],
   "source": [
    "synonyms = get_synonyms(\"active\")\n",
    "print (synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQaPQatb72G9",
    "outputId": "e95545a9-6eab-41f5-cd51-0e113c4739d2"
   },
   "outputs": [],
   "source": [
    "# read the words to list\n",
    "with open('vagueWords.txt') as vague_word_list_file:\n",
    "    vague_word_list = vague_word_list_file.read().splitlines()\n",
    "\n",
    "vague_word_set = set()\n",
    "\n",
    "# Add the synonyms of each word\n",
    "for vagueword in vague_word_list:\n",
    "  for vague_word_synonym in get_synonyms(vagueword):\n",
    "    vague_word_set.add(vague_word_synonym.lower())\n",
    "\n",
    "# Add the antonyms( of each word \n",
    "for vagueword in vague_word_list:\n",
    "  for vague_word_synonym in get_antonyms(vagueword):\n",
    "    vague_word_set.add(vague_word_synonym.lower())\n",
    "\n",
    "# Remove unncessary words\n",
    "remove_word_list = ['adept']\n",
    "for word in remove_word_list:\n",
    "  if word in vague_word_set:\n",
    "    vague_word_set.remove(word)\n",
    "\n",
    "# Add words without adding synonyms\n",
    "addtional_word_list = ['didnt', 'doesnt']\n",
    "for word in addtional_word_list:\n",
    "  vague_word_set.add(word)\n",
    "\n",
    "vague_word_set = sorted(vague_word_set)\n",
    "\n",
    "vague_word_list = list(vague_word_set)\n",
    "\n",
    "print('No. of vague words =', len(vague_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "io0rayZX8W7R",
    "outputId": "9639d434-f6b8-4049-dfde-891207cd452b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "reviews = []\n",
    "with open('reviews.json') as file:\n",
    "  for line in file:\n",
    "      entry = json.loads(line)\n",
    "      reviews.append(entry[\"_source\"][\"review\"])\n",
    "\n",
    "print(\"Total No. of reviews =\", len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJB5ZciJ8bbE"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzmmUaDe8h84"
   },
   "outputs": [],
   "source": [
    "sentences_set = set()\n",
    "for review in reviews:\n",
    "  for sentence in sent_tokenize(review):\n",
    "    sentences_set.add(sentence.lower())\n",
    "sentences = list(sentences_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idJAsof08lbE"
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "  nltk.download('stopwords')\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = set(stopwords.words('english'))\n",
    "  # read the words to list\n",
    "  with open('stopwords.txt') as stop_word_list_file:\n",
    "    for word in stop_word_list_file.read().splitlines():\n",
    "      stopwords.add(word.lower())\n",
    "\n",
    "  additional_words = [\"hi\", \"ok\", \"am\", \"would\", \"i'm\",\"im\",\"ill\",\"cant\",\"else\",\"youd\",\"otherwise\",\"due\"\n",
    "  ,\"youre\",\"ive\",\"havent\",\"hasnt\",\"hadnt\",\"didnt\",\"could\",\"doesnt\",\"may\",\"wouldnt\",\"dont\",\"cant\",\"could\"\n",
    "  ,\"every\",\"anyone\",\"say\",\"isnt\",\"arent\",\"also\",\"cannot\",\"itll\",\"lets\",\"youll\",\"aspacingtopmini\",\"hello\"\n",
    "  ,\"theres\",\"itthe\",\"shes\",\"hes\",\"another\",\"etc\"]\n",
    "  for word in additional_words:\n",
    "    stopwords.add(word)\n",
    "  return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hh5EDe7H9A5Z",
    "outputId": "e432efbe-a6e4-4919-c56a-ec25e2f5c972"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "reviews = sentences\n",
    "reviews = [re.sub(r'[^\\w\\s]','',str(item)) for item in reviews]\n",
    "stopwords = get_stopwords()\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords] for document in reviews]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "         frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming reviews into input data text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gff81PkNA4fN"
   },
   "outputs": [],
   "source": [
    "def write_inputstxt(inputs):\n",
    "  file1 = open(\"input.txt\", \"a\")  # append mode\n",
    "  for text in texts:\n",
    "    file1.write(' '.join(text) + \"\\n\")\n",
    "    \n",
    "  file1.close()\n",
    "write_inputstxt(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = \"15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the Brown Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21jNGvcbzne8",
    "outputId": "67cba678-f1a1-4da6-9cbe-6eac0ca0302c"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.call([\"/static/brown-cluster-master/wcluster\", \"--text\", \"/content/input.txt\", \"--c\", cluster_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the results of the Brown Clustering algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQlf2uvvaFpR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'/content/input-c{cluster_count}-p1.out/paths',  sep='\\t', header=None, names=['cluster', \"word\", \"frequency\"],  dtype={'cluster': str,'word':str, \"frequency\": int},index_col=False)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.sort_values([\"frequency\"], ascending = (False), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6fyzp1F2aFez",
    "outputId": "d91e6d0b-d66b-4b33-f168-777abb7f5132"
   },
   "outputs": [],
   "source": [
    "df.head() #showing sample of the data (first five rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vc-9yjj9gAwE",
    "outputId": "191abfc4-cbdd-4868-f757-c6c6c8709779"
   },
   "outputs": [],
   "source": [
    "print(df.cluster.unique(), len(df.cluster.unique()))#Unique clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the words frequencies in a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw33T8Dlfjdb"
   },
   "outputs": [],
   "source": [
    "def get_words(cluster, df):\n",
    "  df1 = df[df[\"cluster\"] == cluster]\n",
    "  return df1.set_index(\"word\").to_dict()['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_words(\"000\", df).items()) #example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3s2NXIwBeT7T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import json\n",
    "import math\n",
    "import re # We clean text using regex\n",
    "import csv # To read the csv\n",
    "from collections import defaultdict # For accumlating values\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "\n",
    "# Start with loading all necessary libraries\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vOKVM7a6ZZGu",
    "outputId": "d3fa05f4-1bfd-406f-ca5d-112da19eee16"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for cluster_index, cluster_value in enumerate(df.cluster.unique()):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud(max_font_size=50, max_words=100, background_color=\"white\").fit_words(get_words(cluster_value,df)),interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Cluster #\" + str(cluster_index+1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing pandas dataframe for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2dshFdJ_loV"
   },
   "outputs": [],
   "source": [
    "def get_topics(df):\n",
    "  my_list = []\n",
    "  for i, value in enumerate(df.cluster.unique()):\n",
    "    a_view = get_words(value, df).items()\n",
    "    a_list = list(a_view)\n",
    "    my_tuple = (i, a_list[:20])\n",
    "    my_list.append(my_tuple)\n",
    "  return my_list\n",
    "\n",
    "topics = get_topics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Vagueness degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnO8IaQgMVV2",
    "outputId": "8086af98-edc6-4da2-ae2d-830ac3fa762c"
   },
   "outputs": [],
   "source": [
    "def sum_of_frequency_of_words_in_topic(topic_index, topics):\n",
    "  sum = 0\n",
    "  for index, value in enumerate(topics[topic_index][1]):\n",
    "    sum += value[1]\n",
    "    # print(value)\n",
    "  return sum\n",
    "\n",
    "def sum_of_frequency_of_vague_words_in_topic(vague_word_list, topic_index, topics):\n",
    "  sum = 0\n",
    "  for index, value in enumerate(topics[topic_index][1]):\n",
    "    if value[0] in vague_word_list:\n",
    "      sum += value[1]\n",
    "      # print(value)\n",
    "  return sum  \n",
    "\n",
    "def vagueness_degree(vague_word_list):\n",
    "  for i in range(len(topics)):\n",
    "    all_word_count = sum_of_frequency_of_words_in_topic(i, topics)\n",
    "    vague_word_count = sum_of_frequency_of_vague_words_in_topic(vague_word_list, i, topics)\n",
    "    percentage = round(vague_word_count / all_word_count * 10000) / 100\n",
    "    percentage_str = \"percentage: \" + str(percentage) + \"%\"\n",
    "    if percentage >= 10.00:\n",
    "      print(\"topic\", f'{i:<2}', f'{\"vague: \" + str(vague_word_count):<15}', f'{\"all: \" + str(all_word_count):<15}', f'{percentage_str:<20}', \"vague\")\n",
    "    else:\n",
    "      print(\"topic\", f'{i:<2}', f'{\"vague: \" + str(vague_word_count):<15}', f'{\"all: \" + str(all_word_count):<15}', f'{percentage_str:<20}')\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "vagueness_degree(vague_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "a7RJX-yllHmB",
    "outputId": "f4f04044-ca50-44f6-c0cf-0232d714a1a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_topic_percentage(topics):\n",
    "  topic_rankings_x = []\n",
    "  topic_rankings_y = []\n",
    "  for i in range(len(topics)):\n",
    "    all_word_count = sum_of_frequency_of_words_in_topic(i, topics)\n",
    "    vague_word_count = sum_of_frequency_of_vague_words_in_topic(vague_word_list, i, topics)\n",
    "    percentage = round(vague_word_count / all_word_count * 100, 2)\n",
    "    topic_rankings_x.append(i + 1)\n",
    "    topic_rankings_y.append(percentage)\n",
    "    # print(percentage)\n",
    "\n",
    "  plt.yticks(np.arange(0, 100, 10))\n",
    "  plt.xticks(np.arange(1, len(topics) + 1, 1))\n",
    "  plt.bar(topic_rankings_x, topic_rankings_y)\n",
    "  plt.ylabel('Vagueness Percentage')\n",
    "  plt.xlabel('Topics')\n",
    "  plt.show()\n",
    "plot_topic_percentage(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words in vague topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDx9QbM1XBI5",
    "outputId": "1ed65995-cd39-4e9a-fcf5-fc376f00eceb"
   },
   "outputs": [],
   "source": [
    "def print_vague_topic_words(vague_word_list):\n",
    "  for i in range(len(topics)):\n",
    "    all_word_count = sum_of_frequency_of_words_in_topic(i, topics)\n",
    "    vague_word_count = sum_of_frequency_of_vague_words_in_topic(vague_word_list, i, topics)\n",
    "    percentage = round(vague_word_count / all_word_count * 10000) / 100\n",
    "    if percentage >= 10.00:\n",
    "      print(\"\\ntopic\", i)\n",
    "      for index, value in enumerate(topics[i][1]):\n",
    "        print(\"             \" + f'{value[0]:<14}', value[1])\n",
    "print_vague_topic_words(vague_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot of top 20 words with their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X2TEdoNB69GT",
    "outputId": "b5eab76d-ba0e-444d-eb05-537fc318f446",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "def plot_words_in_topic(topics):\n",
    "  for i in range(len(topics)):\n",
    "    figure(figsize=(20, 6), dpi=80)\n",
    "    y_ticks = []\n",
    "    x_labels = []\n",
    "\n",
    "    # print(topics[i][1])\n",
    "    for key, value in enumerate(topics[i][1]):\n",
    "      # print(value)\n",
    "      y_ticks.append(value[1])\n",
    "      x_labels.append(value[0])\n",
    "\n",
    "    x_ticks = list(range(1, (len(x_labels) + 1)))\n",
    "    plt.xticks(x_ticks, x_labels)\n",
    "    plt.bar(x_ticks, y_ticks, width=.2)\n",
    "    plt.ylabel('Word frequency')\n",
    "    plt.xlabel('Topic #' + str(i + 1))\n",
    "    plt.show()\n",
    "    print()\n",
    "    print()\n",
    "plot_words_in_topic(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_labels(df):\n",
    "  df1 = df\n",
    "  df1['row_number'] = df.groupby(['cluster']).cumcount() + 1\n",
    "  df1 = df1[df1['row_number'] == 1] \n",
    "  return df1\n",
    "cluster_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, key=\"\", val=\"\", left=None, right=None, parent=None):\n",
    "        self.key = key\n",
    "        self.val = val\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "        self.parent = parent\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'k: {self.key}, v: {self.val}'\n",
    "\n",
    "def get_items(df):\n",
    "  items = {}\n",
    "  for index, row in df.iterrows():\n",
    "    items[row['cluster']] = row['word']\n",
    "  return items\n",
    "\n",
    "items = get_items(cluster_labels(df))\n",
    "root = TreeNode(\"root\")\n",
    "\n",
    "\n",
    "def get_TreeNode(tree_node, key, value):\n",
    "    current_node = tree_node\n",
    "    for i, v in enumerate(key):\n",
    "        partial_key = key[0:i + 1]\n",
    "        if v == \"0\":\n",
    "            if current_node.right is None:\n",
    "                current_node.right = TreeNode(key=partial_key, parent=current_node)\n",
    "            current_node = current_node.right\n",
    "        else:\n",
    "            if current_node.left is None:\n",
    "                current_node.left = TreeNode(key=partial_key, parent=current_node)\n",
    "            current_node = current_node.left\n",
    "    current_node.val = value\n",
    "    current_node.key = key\n",
    "    return current_node\n",
    "\n",
    "\n",
    "for key, value in items.items():\n",
    "    get_TreeNode(root, key, value)\n",
    "\n",
    "\n",
    "def parent_list_graphviz_str(parents):\n",
    "    parents = parents[::-1]\n",
    "    sj = []\n",
    "    for item in parents:\n",
    "        if item.val == \"\":\n",
    "            sj.append(f'\"{item.key}\"')\n",
    "        else:\n",
    "            sj.append(f'\"{item.key}:{item.val}\"')\n",
    "    return ' -> '.join(sj)\n",
    "\n",
    "\n",
    "def get_parent_list(current):\n",
    "    parents = [current]\n",
    "    parent = current\n",
    "    while parent.parent is not None:\n",
    "        parents.append(parent.parent)\n",
    "        parent = parent.parent\n",
    "    return parents\n",
    "\n",
    "ranks = {999: []}\n",
    "\n",
    "\n",
    "def append_to_ranks(ranks, current):\n",
    "    if current.key == \"root\":\n",
    "        return\n",
    "    if current.left is None and current.right is None:\n",
    "        ranks[999].append(f'\"{current.key}:{current.val}\"')\n",
    "    else:\n",
    "        if len(current.key) not in ranks:\n",
    "            ranks[len(current.key)] = []\n",
    "        ranks[len(current.key)].append(f'\"{current.key}\"')\n",
    "\n",
    "\n",
    "graphviz_source = ['strict digraph { node [shape=box]']\n",
    "graphviz_source.append('graph [splines=ortho]')\n",
    "stack = [root]\n",
    "while stack:\n",
    "    current = stack.pop()\n",
    "    append_to_ranks(ranks, current)\n",
    "\n",
    "    if current.val != \"\":\n",
    "        parents = get_parent_list(current)\n",
    "        graphviz_source.append(parent_list_graphviz_str(parents))\n",
    "\n",
    "    if current.right is not None:\n",
    "        stack.append(current.right)\n",
    "\n",
    "    if current.left is not None:\n",
    "        stack.append(current.left)\n",
    "\n",
    "for value in sorted(ranks.keys()):\n",
    "  graphviz_source.append('{rank = same; ' + \"; \".join(ranks[value]) + '}')        \n",
    "\n",
    "for str in graphviz_source:\n",
    "    print(str)\n",
    "\n",
    "graphviz_source.append(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = Source('\\n'.join(graphviz_source))\n",
    "src"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Brown Clustering percyliang.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
