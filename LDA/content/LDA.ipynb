{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXcxYHcxjmZm"
   },
   "source": [
    "# Importing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Q0_XfF_PHxU",
    "outputId": "5086910f-0590-43cb-e492-74e43fbb43ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kAZduDd4NUP"
   },
   "source": [
    "# Prepare vague words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QC3BMrTHyqw9"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEV262efzZIS"
   },
   "outputs": [],
   "source": [
    "def get_synonyms(ofword):\n",
    "  from nltk.corpus import wordnet\n",
    "  synonyms = []\n",
    "\n",
    "  for syn in wordnet.synsets(ofword):\n",
    "    for l in syn.lemmas():\n",
    "      synonyms.append(l.name())\n",
    "\n",
    "  return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmVg1Zdw3hZY"
   },
   "outputs": [],
   "source": [
    "def get_antonyms(ofword):\n",
    "  from nltk.corpus import wordnet\n",
    "  antonyms = []\n",
    "\n",
    "  for syn in wordnet.synsets(ofword):\n",
    "    for l in syn.lemmas():\n",
    "      if l.antonyms():\n",
    "          antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "  return antonyms\n",
    "  # print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9bwfc9l0tzs",
    "outputId": "35f0e127-dde0-4d55-b090-95a5ac228fcc"
   },
   "outputs": [],
   "source": [
    "synonyms = get_synonyms(\"active\")\n",
    "print (synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsf8Sf8z1H2J",
    "outputId": "f03d41a7-9324-4bd5-e194-a8396440d701"
   },
   "outputs": [],
   "source": [
    "# read the words to list\n",
    "with open('vagueWords.txt') as vague_word_list_file:\n",
    "    vague_word_list = vague_word_list_file.read().splitlines()\n",
    "\n",
    "vague_word_set = set()\n",
    "\n",
    "# Add the synonyms of each word\n",
    "for vagueword in vague_word_list:\n",
    "  for vague_word_synonym in get_synonyms(vagueword):\n",
    "    vague_word_set.add(vague_word_synonym.lower())\n",
    "\n",
    "# Add the antonyms( of each word \n",
    "for vagueword in vague_word_list:\n",
    "  for vague_word_synonym in get_antonyms(vagueword):\n",
    "    vague_word_set.add(vague_word_synonym.lower())\n",
    "\n",
    "# Remove unncessary words\n",
    "remove_word_list = ['adept']\n",
    "for word in remove_word_list:\n",
    "  if word in vague_word_set:\n",
    "    vague_word_set.remove(word)\n",
    "\n",
    "# Add words without adding synonyms\n",
    "addtional_word_list = ['didnt', 'doesnt']\n",
    "for word in addtional_word_list:\n",
    "  vague_word_set.add(word)\n",
    "\n",
    "vague_word_set = sorted(vague_word_set)\n",
    "\n",
    "vague_word_list = list(vague_word_set)\n",
    "\n",
    "print(\"No. of Vague words\", len(vague_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guY84Gn5j59I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import json\n",
    "import math\n",
    "import re # We clean text using regex\n",
    "import csv # To read the csv\n",
    "from collections import defaultdict # For accumlating values\n",
    "from nltk.corpus import stopwords # To remove stopwords\n",
    "from gensim import corpora # To create corpus and dictionary for the LDA model\n",
    "from gensim.models import LdaModel # To use the LDA model\n",
    "import pyLDAvis.gensim # To visualise LDA model effectively\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "\n",
    "# Start with loading all necessary libraries\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IirhQwXS1mjt"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8KndJ-HkZ0P"
   },
   "source": [
    "# Importing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykVLCqklkpWJ",
    "outputId": "af5ac275-78a1-4344-d591-9ec0208fe718"
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "with open('reviews.json') as file:\n",
    "  for line in file:\n",
    "      entry = json.loads(line)\n",
    "      reviews.append(entry[\"_source\"][\"review\"])\n",
    "print(\"Total No. of reviews =\", len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small part of the reviews for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews = reviews[:1405] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lengths = []\n",
    "average_review_length = 0\n",
    "total_length = 0\n",
    "min_length = len(reviews[0])\n",
    "max_length = 0\n",
    "for i, review in enumerate(reviews):\n",
    "  review_length = len(review)\n",
    "  total_length = total_length + review_length\n",
    "  review_lengths.append(review_length)\n",
    "  if max_length < review_length:\n",
    "    max_length = review_length\n",
    "  if min_length > review_length:\n",
    "    min_length = review_length\n",
    "\n",
    "average_review_length = math.ceil(total_length/len(reviews))\n",
    "print(\"Total Reviews: \"+str(len(reviews))+ \", Average Review Length:\"+ str(average_review_length)+\", Minimum Length: \"+str(min_length)+\", Maximum Length: \"+str(max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW2I1OdWn02H"
   },
   "source": [
    "# Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QDBMeNElzxU"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytek9AZRld0B"
   },
   "outputs": [],
   "source": [
    "sentences_set = set()\n",
    "for review in reviews:\n",
    "  for sentence in sent_tokenize(review):\n",
    "    sentences_set.add(sentence.lower())\n",
    "sentences = list(sentences_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LceuZLtnQ_L5"
   },
   "source": [
    "#  Cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikNTQRxPpZ7e"
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "  nltk.download('stopwords')\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = set(stopwords.words('english'))\n",
    "  # read the words to list\n",
    "  with open('stopwords.txt') as stop_word_list_file:\n",
    "    for word in stop_word_list_file.read().splitlines():\n",
    "      stopwords.add(word.lower())\n",
    "\n",
    "  additional_words = [\"hi\", \"ok\", \"am\", \"would\", \"i'm\",\"im\",\"ill\",\"cant\",\"else\",\"youd\",\"otherwise\",\"due\"\n",
    "  ,\"youre\",\"ive\",\"havent\",\"hasnt\",\"hadnt\",\"didnt\",\"could\",\"doesnt\",\"may\",\"wouldnt\",\"dont\",\"cant\",\"could\"\n",
    "  ,\"every\",\"anyone\",\"say\",\"isnt\",\"arent\",\"also\",\"cannot\",\"itll\",\"lets\",\"youll\",\"aspacingtopmini\",\"hello\"\n",
    "  ,\"theres\",\"itthe\",\"shes\",\"hes\",\"another\",\"etc\"]\n",
    "\n",
    "  for word in additional_words:\n",
    "    stopwords.add(word)\n",
    "  return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-StOYCIalSvC",
    "outputId": "f9b2f4d8-6050-445d-fc18-0f6c54c4d630"
   },
   "outputs": [],
   "source": [
    "reviews = sentences\n",
    "reviews = [re.sub(r'[^\\w\\s]','',str(item)) for item in reviews]\n",
    "stopwords = get_stopwords()\n",
    "texts = [[word for word in document.lower().split() if word not in stopwords] for document in reviews]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "         frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxCoerWtp5cc"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmuF_olGpaDQ"
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFSbVv8URGvm"
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ay7dJUXqLZa"
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 12\n",
    "ldamodel = LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, random_state=10, passes=15, alpha=0.01, eta=0.001)\n",
    "topics = ldamodel.show_topics(num_topics= NUM_TOPICS,num_words=20,formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQGVjKOl_GDL"
   },
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOqg1zvTbdYG",
    "outputId": "0349e6cc-d9f3-4e95-f614-abf4778d1fb5"
   },
   "outputs": [],
   "source": [
    "def sum_of_probability_of_words_in_topic(topic_index, topics):\n",
    "  sum = 0\n",
    "  for index, value in enumerate(topics[topic_index][1]):\n",
    "    sum += value[1]\n",
    "    # print(value)\n",
    "  return sum\n",
    "\n",
    "def sum_of_probability_of_vague_words_in_topic(vague_word_list, topic_index, topics):\n",
    "  sum = 0\n",
    "  for index, value in enumerate(topics[topic_index][1]):\n",
    "    if value[0] in vague_word_list:\n",
    "      sum += value[1]\n",
    "      # print(value)\n",
    "  return sum  \n",
    "\n",
    "def vagueness_degree(vague_word_list):\n",
    "  for i in range(len(topics)):\n",
    "    all_word_probability = sum_of_probability_of_words_in_topic(i, topics)\n",
    "    vague_word_probability = sum_of_probability_of_vague_words_in_topic(vague_word_list, i, topics)\n",
    "    percentage = round(vague_word_probability / all_word_probability * 100, 2)\n",
    "    percentage_str = \"percentage: \" + str(percentage) + \"%\"\n",
    "    if percentage >= 10.00:\n",
    "      print(\"topic\", f'{i+1:<2}', f'{\"vague: \" + str(round(vague_word_probability, 4)):<15}', f'{\"probability sum: \" + str(round(all_word_probability, 4)):<25}', f'{percentage_str:<20}', \"vague\")\n",
    "    else:\n",
    "      print(\"topic\", f'{i+1:<2}', f'{\"vague: \" + str(round(vague_word_probability, 4)):<15}', f'{\"probability sum: \" + str(round(all_word_probability, 4)):<25}', f'{percentage_str:<20}')\n",
    "\n",
    "    \n",
    "\n",
    "vagueness_degree(vague_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpTpUkWXdQ_d"
   },
   "outputs": [],
   "source": [
    "def print_vague_topic_words(vague_word_list):\n",
    "  for i in range(len(topics)):\n",
    "    all_word_probability = sum_of_probability_of_words_in_topic(i, topics)\n",
    "    vague_word_probability = sum_of_probability_of_vague_words_in_topic(vague_word_list, i, topics)\n",
    "    percentage = round(vague_word_probability / all_word_probability * 10000) / 100\n",
    "    if percentage >= 10.00:\n",
    "      print(\"\\ntopic\", i+1)\n",
    "      for index, value in enumerate(topics[i][1]):\n",
    "        print(\"             \" + f'{value[0]:<14}', value[1])\n",
    "print_vague_topic_words(vague_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "n2U5mWunfc7B",
    "outputId": "936027cd-8678-4750-9513-c086d7049b72"
   },
   "outputs": [],
   "source": [
    "def plot_topic_percentage(topics):\n",
    "\n",
    "  topic_rankings_x = []\n",
    "  topic_rankings_y = []\n",
    "  for i in range(len(topics)):\n",
    "    all_word_count = sum_of_probability_of_words_in_topic(i, topics)\n",
    "    vague_word_count = sum_of_probability_of_vague_words_in_topic(vague_word_list, i, topics)\n",
    "    percentage = round(vague_word_count / all_word_count * 100, 2)\n",
    "    topic_rankings_x.append(i + 1)\n",
    "    topic_rankings_y.append(percentage)\n",
    "    # print(percentage)\n",
    "\n",
    "  plt.yticks(np.arange(0, 100, 10))\n",
    "  plt.xticks(np.arange(1, NUM_TOPICS + 1, 1))\n",
    "  plt.bar(topic_rankings_x, topic_rankings_y)\n",
    "  plt.ylabel('Vagueness Percentage')\n",
    "  plt.xlabel('Topics')\n",
    "  plt.show()\n",
    "plot_topic_percentage(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1YIhSGdiU9S",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for t in range(ldamodel.num_topics):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud(max_font_size=50, max_words=100, background_color=\"white\").fit_words(dict(ldamodel.show_topic(t, 200))),interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t+1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar plot for top 20 words weights in each topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dRf9vJMKpOBw",
    "outputId": "962be845-0496-4648-a366-ef24f56df61b"
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "def plot_words_in_topic(topics):\n",
    "  for i in range(len(topics)):\n",
    "    figure(figsize=(20, 6), dpi=80)\n",
    "    y_ticks = []\n",
    "    x_labels = []\n",
    "    # print(topics[i][1])\n",
    "    for key, value in enumerate(topics[i][1]):\n",
    "      # print(value)\n",
    "      y_ticks.append(value[1])\n",
    "      x_labels.append(value[0])\n",
    "    # plt.yticks(np.arange(0, 1, .1))\n",
    "    x_ticks = list(range(1, (len(x_labels) + 1)))\n",
    "    plt.xticks(x_ticks, x_labels)\n",
    "    plt.bar(x_ticks, y_ticks, width=.2)\n",
    "    plt.ylabel('Word probabilty')\n",
    "    plt.xlabel('Topic #' + str(i + 1))\n",
    "    plt.show()\n",
    "    print()\n",
    "    print()\n",
    "plot_words_in_topic(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 Words in each topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXnVgFFvujis"
   },
   "outputs": [],
   "source": [
    "word_dict = {};\n",
    "for i in range(NUM_TOPICS):\n",
    "    words = ldamodel.show_topic(i, topn = 20)\n",
    "    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n",
    "pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f59PZRFfqXta"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTIshFODunUr",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimum number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading LDA mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJMWnNWiet77"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.models\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    mallet_path = '/static/mallet-2.0.8/bin/mallet' # use it on google colab\n",
    "    print('before running ', start, ' ', limit, ' ', step)\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print('running')\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, random_seed=10, iterations=1, id2word=ldamodel.id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XuQTNq31u3h"
   },
   "outputs": [],
   "source": [
    "start=9; limit=20; step=1;\n",
    "#model_list, coherence_values = compute_coherence_values(dictionary=ldamodel.id2word, corpus=corpus, texts=texts, start=start, limit=limit, step=step)\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=start, limit=limit, step=step)\n",
    "# Show graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qplM-vDm1mkh"
   },
   "outputs": [],
   "source": [
    "coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJCV9InGASkg"
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpUXqhtuAbrq",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "best_topic_no = 0\n",
    "best_topic_cv = 0\n",
    "\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    if cv > best_topic_cv + 0.005:\n",
    "        best_topic_cv = cv\n",
    "        best_topic_no = m\n",
    "print('Best coherence topic ', best_topic_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimum values of Alpha and Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSsWNjs1JPGF"
   },
   "outputs": [],
   "source": [
    "# supporting function to find alpha and beta\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    print(\"lda_model = gensim.models.LdaMulticore(corpus=corpus,\")\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           workers=4,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=10,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                               eta=b)\n",
    "    print(\"coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=texts, dictionary=ldamodel.id2word, coherence='c_v')\")\n",
    "    coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    print(\"return coherence_model_lda.get_coherence()\")\n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nARH5MFmJUFi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = best_topic_no\n",
    "max_topics = min_topics + 1\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.5))\n",
    "#alpha.append('symmetric')\n",
    "#alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.5))\n",
    "#beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               # gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "            \n",
    "corpus_title = [#'25% Corpus',\n",
    "                #'50% Corpus',\n",
    "                 '75% Corpus',\n",
    "                '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "                \n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    #pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        print(\"i \" + str(i))\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            print(\"k \"  + str(k))\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                print(\"a \" + str(a))\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    print(\"b \" + str(b))\n",
    "                    print(\"# get the coherence score for the given parameters\")\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, k=k, a=a, b=b)\n",
    "                    print(\"# Save the model results\")\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    " #                   pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "  #  pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUXBU_vpogAV"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "model_results_df = pd.DataFrame(model_results).sort_values(by=['Coherence'], ascending=[False]).reset_index(drop=True)\n",
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5vKCW8sA_zu"
   },
   "outputs": [],
   "source": [
    "print(\"The recommended nubmer of topics is\", model_results_df['Topics'][0], \"with value of Alpha=\", model_results_df['Alpha'][0], \"and Eta=\", model_results_df['Beta'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h97rXp6mP1-i"
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "diamond hakwna matata.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
